#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­åŒ»è¾¨è¯è®ºæ²»Benchmarkè¯„æµ‹æ¡†æ¶

ä¸»è¦åŠŸèƒ½ï¼š
1. å¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œä¸­åŒ»è¾¨è¯è®ºæ²»èƒ½åŠ›è¯„æµ‹
2. æ”¯æŒ14ä¸ªç»´åº¦çš„ç»¼åˆè¯„ä¼°
3. åŒ…å«é€‰æ‹©é¢˜ã€å¥–åŠ±æ¨¡å‹è¯„åˆ†ã€LLMåˆ¤åˆ†ç­‰å¤šç§è¯„æµ‹æ–¹å¼

Usage:
    python tcm_benchmark.py --model_type api --api_url http://localhost:8000/v1 --model_name gpt-4
    python tcm_benchmark.py --model_type local --model_path /path/to/model
"""

import json
import random
import time
import logging
import argparse
import os
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from tqdm import tqdm
import numpy as np
from datetime import datetime

# å¯¼å…¥å„ä¸ªè¯„æµ‹æ¨¡å—
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data_loader import TCMDataLoader
from evaluators import (
    MultipleChoiceEvaluator,
    RewardModelEvaluator,
    LLMJudgeEvaluator,
)
from model_interface import ModelInterface
from report_generator import ReportGenerator
from utils import setup_logging, save_checkpoint, load_checkpoint

# é…ç½®æ—¥å¿—
setup_logging()
logger = logging.getLogger(__name__)

# æ˜¾ç¤ºåæ˜ å°„ï¼ˆä»…ç”¨äºè¾“å‡º/æŠ¥å‘Šï¼Œä¸æ”¹å˜å†…éƒ¨è¯„æµ‹é”®ï¼‰
DISPLAY_NAME_MAP = {
    "å®‰å…¨é—®é¢˜": "å¤§æ¨¡å‹å†…å®¹å®‰å…¨",
}

@dataclass
class EvaluationConfig:
    """è¯„æµ‹é…ç½®"""
    # æ•°æ®è·¯å¾„
    data_path: str = "benchmark.json"
    
    # æ¨¡å‹é…ç½®
    local_model_gpu_id: int = -1
    reward_api_host: str = "36.103.196.56"
    reward_api_port: int = 8001
    llm_judge_api_host: str = "36.103.196.56"
    llm_judge_api_port: int = 8002
    # å¯é…ç½®çš„æ¨¡å‹åç§°
    reward_model_name: str = "fangji"
    llm_judge_model_name: str = "qwen"
    # API Keysï¼ˆç”¨äºOpenAIå…¼å®¹æœåŠ¡ï¼‰
    reward_api_key: str = ""
    llm_judge_api_key: str = ""
    
    # è¯„æµ‹é…ç½®
    random_seed: Optional[int] = None  # Noneè¡¨ç¤ºè‡ªåŠ¨ç”Ÿæˆéšæœºç§å­
    max_retries: int = 3
    checkpoint_interval: int = 10

    stop_on_model_error: bool = True
    fatal_error_keywords: List[str] = field(default_factory=lambda: [
        "AccountOverdueError", "insufficient_quota", "insufficient quota", "quota exceeded",
        "Forbidden", "Error code: 403", "403", "rate limit", "too many requests",
        "unauthorized", "invalid_api_key", "notfound", "not found", "the model", "does not exist"
    ])
    
    def __post_init__(self):
        # å¦‚æœæ²¡æœ‰æŒ‡å®šéšæœºç§å­ï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆ
        if self.random_seed is None:
            # ä½¿ç”¨å½“å‰æ—¶é—´æˆ³ + è¿›ç¨‹ID + éšæœºæ•°ç”ŸæˆçœŸæ­£éšæœºçš„ç§å­
            import os
            timestamp = int(datetime.now().timestamp() * 1000000)  # å¾®ç§’çº§æ—¶é—´æˆ³
            pid = os.getpid()  # è¿›ç¨‹ID
            rand_component = random.randint(0, 999999)  # é¢å¤–éšæœºåˆ†é‡
            self.random_seed = (timestamp + pid + rand_component) % (2**31 - 1)
            logger.info(f"è‡ªåŠ¨ç”Ÿæˆéšæœºç§å­: {self.random_seed}")

class TCMBenchmark:
    """ä¸­åŒ»è¾¨è¯è®ºæ²»Benchmarkä¸»ç±»"""
    
    def __init__(self, config: EvaluationConfig, skip_think: bool = False):
        """
        åˆå§‹åŒ–è¯„æµ‹å™¨
        
        Args:
            config: è¯„æµ‹é…ç½®
            skip_think: æ˜¯å¦è·³è¿‡Thinkå†…å®¹å®Œå¤‡æ€§è¯„æµ‹
        """
        self.config = config
        self.skip_think = skip_think
        
        # è®¾ç½®éšæœºç§å­ï¼ˆç¡®ä¿å¯é‡ç°æ€§ï¼‰
        random.seed(config.random_seed)
        np.random.seed(config.random_seed)
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.data_loader = TCMDataLoader(config.data_path)
        self.multiple_choice_evaluator = MultipleChoiceEvaluator(config.random_seed)
        # ä¸å†ä½¿ç”¨BGEæ–‡æœ¬ç›¸ä¼¼åº¦è¯„æµ‹å™¨ï¼Œæ”¹ä¸ºç”±LLMè¿›è¡Œç—…å› /ç—…æœºåˆ¤åˆ†
        self.text_similarity_evaluator = None
        self.reward_model_evaluator = RewardModelEvaluator(
            config.reward_api_host,
            config.reward_api_port,
            config.reward_model_name,
            api_key=getattr(config, 'reward_api_key', None)
        )
        self.llm_judge_evaluator = LLMJudgeEvaluator(
            config.llm_judge_api_host,
            config.llm_judge_api_port,
            config.llm_judge_model_name,
            api_key=getattr(config, 'llm_judge_api_key', None)
        )
        self.report_generator = ReportGenerator()
        
        # è¯„æµ‹ç»´åº¦ï¼ˆå›ºå®šé¡ºåºï¼‰
        base_dimensions = [
            "è¯å‹", "ç—…æ€§", "ç—…ä½", "æ²»åˆ™æ²»æ³•",  # å®¢è§‚é¢˜è¯„æµ‹
            "ç—…å› ", "ç—…æœº",  # LLMè¯„åˆ†
            "æ–¹è¯å¥‘åˆåº¦", "æ–¹å‰‚é…ä¼è§„å¾‹", "é…ä¼ç¦å¿Œ", "è¯æå®‰å…¨æ€§åˆ†æ", "å¦Šå¨ ç¦å¿Œ",  # å¤„æ–¹ç›¸å…³ï¼ˆæ–¹è¯å¥‘åˆåº¦ä¸ºå¥–åŠ±æ¨¡å‹è¯„åˆ†ï¼Œå…¶ä½™å½’å± LLM è¯„åˆ†æ˜¾ç¤ºï¼‰
            "ç…æœæ–¹æ³•", "æ³¨æ„äº‹é¡¹", "éšç—‡åŠ å‡"  # LLMè¯„åˆ†
        ]
        
        # æ ¹æ®skip_thinkå‚æ•°å†³å®šæ˜¯å¦åŒ…å«CoTå†…å®¹å®Œå¤‡æ€§å’ŒCoTå‡†ç¡®æ€§
        if not self.skip_think:
            # åœ¨LLMåˆ¤åˆ†ç»´åº¦çš„å¼€å¤´æ’å…¥CoTå†…å®¹å®Œå¤‡æ€§å’ŒCoTå‡†ç¡®æ€§
            self.evaluation_dimensions = base_dimensions[:11] + ["CoTå†…å®¹å®Œå¤‡æ€§", "CoTå‡†ç¡®æ€§"] + base_dimensions[11:]
        else:
            self.evaluation_dimensions = base_dimensions
            
        logger.info(f"è¯„æµ‹ç»´åº¦æ•°é‡: {len(self.evaluation_dimensions)}")
        if self.skip_think:
            logger.info("å·²è·³è¿‡CoTå†…å®¹å®Œå¤‡æ€§å’ŒCoTå‡†ç¡®æ€§è¯„æµ‹")
        
        # å­˜å‚¨è¯¦ç»†ç»“æœ
        self.detailed_results: List[Dict[str, Any]] = []
        # å­˜å‚¨é€šç”¨è¯„æµ‹ä»»åŠ¡ï¼ˆåŸæ–°å¢ç±»åˆ«ï¼‰ç»“æœ
        self.general_assessment_task_results: Dict[str, List[Dict[str, Any]]] = {
            "ä¸­åŒ»åŸºç¡€çŸ¥è¯†": [],
            "åŒ»å­¦ä¼¦ç†": [],
            "å®‰å…¨é—®é¢˜": []
        }

    def run_evaluation(self, model_interface: ModelInterface, 
                      output_dir: str = "benchmark/results",
                      resume_from_checkpoint: bool = True) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´è¯„æµ‹
        
        Args:
            model_interface: å¾…è¯„æµ‹æ¨¡å‹æ¥å£
            output_dir: è¾“å‡ºç›®å½•
            resume_from_checkpoint: æ˜¯å¦ä»æ–­ç‚¹æ¢å¤
            
        Returns:
            è¯„æµ‹ç»“æœå­—å…¸
        """
        logger.info("å¼€å§‹ä¸­åŒ»è¾¨è¯è®ºæ²»Benchmarkè¯„æµ‹")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        checkpoint_path = os.path.join(output_dir, "checkpoint.json")
        
        # åŠ è½½æ•°æ®
        cases = self.data_loader.load_cases()
        logger.info(f"åŠ è½½äº† {len(cases)} ä¸ªæ¡ˆä¾‹")
        
        # æŒ‰ç±»åˆ«åˆ†ç±»æ•°æ®
        tcm_cases = [case for case in cases if case.get("class") == "ä¸­åŒ»è¾¨è¯è®ºæ²»"]
        basic_cases = [case for case in cases if case.get("class") == "ä¸­åŒ»åŸºç¡€çŸ¥è¯†"]
        ethics_cases = [case for case in cases if case.get("class") == "åŒ»å­¦ä¼¦ç†"]
        safety_cases = [case for case in cases if case.get("class") == "å®‰å…¨é—®é¢˜"]
        
        logger.info(f"ä¸­åŒ»è¾¨è¯è®ºæ²»: {len(tcm_cases)} ä¸ªæ¡ˆä¾‹")
        logger.info(f"ä¸­åŒ»åŸºç¡€çŸ¥è¯†: {len(basic_cases)} ä¸ªæ¡ˆä¾‹")
        logger.info(f"åŒ»å­¦ä¼¦ç†: {len(ethics_cases)} ä¸ªæ¡ˆä¾‹")
        logger.info(f"å®‰å…¨é—®é¢˜: {len(safety_cases)} ä¸ªæ¡ˆä¾‹")
        
        # å°è¯•ä»æ–­ç‚¹æ¢å¤
        start_idx = 0
        if resume_from_checkpoint and os.path.exists(checkpoint_path):
            checkpoint_data = load_checkpoint(checkpoint_path)
            if checkpoint_data:
                start_idx = checkpoint_data.get("completed_cases", 0)
                self.detailed_results = checkpoint_data.get("detailed_results", [])
                # æ¢å¤é€šç”¨è¯„æµ‹ä»»åŠ¡ç»“æœï¼ˆä¸å…¼å®¹æ—§é”®ï¼‰
                self.general_assessment_task_results = checkpoint_data.get("general_assessment_task_results", {
                    "ä¸­åŒ»åŸºç¡€çŸ¥è¯†": [],
                    "åŒ»å­¦ä¼¦ç†": [],
                    "å®‰å…¨é—®é¢˜": []
                })
                # æ¢å¤éšæœºç§å­
                if "random_seed" in checkpoint_data:
                    self.config.random_seed = checkpoint_data["random_seed"]
                logger.info(f"ä»æ–­ç‚¹æ¢å¤ï¼Œå·²å®Œæˆ {start_idx} ä¸ªæ¡ˆä¾‹")
        
        # ä½¿ç”¨éšæœºç§å­è¿›è¡Œæ¡ˆä¾‹é¡ºåºéšæœºåŒ–
        random.seed(self.config.random_seed)
        random.shuffle(tcm_cases)
        random.shuffle(basic_cases)
        random.shuffle(ethics_cases)
        random.shuffle(safety_cases)
        logger.info(f"ä½¿ç”¨éšæœºç§å­ {self.config.random_seed} è¿›è¡Œæ¡ˆä¾‹é¡ºåºéšæœºåŒ–")
        
        # è¯„æµ‹ä¸­åŒ»è¾¨è¯è®ºæ²»ç±»åˆ«ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        if len(tcm_cases) > 0:
            logger.info("å¼€å§‹è¯„æµ‹ä¸­åŒ»è¾¨è¯è®ºæ²»ç±»åˆ«")
            self._evaluate_tcm_cases(tcm_cases, model_interface, output_dir, resume_from_checkpoint)
        
        # è¯„æµ‹é€šç”¨è¯„æµ‹ä»»åŠ¡ï¼ˆåŸæ–°å¢ç±»åˆ«ï¼‰
        if len(basic_cases) > 0:
            logger.info("å¼€å§‹è¯„æµ‹ä¸­åŒ»åŸºç¡€çŸ¥è¯†ç±»åˆ«")
            self._evaluate_new_class_cases("ä¸­åŒ»åŸºç¡€çŸ¥è¯†", basic_cases, model_interface, output_dir)
            
        if len(ethics_cases) > 0:
            logger.info("å¼€å§‹è¯„æµ‹åŒ»å­¦ä¼¦ç†ç±»åˆ«")
            self._evaluate_new_class_cases("åŒ»å­¦ä¼¦ç†", ethics_cases, model_interface, output_dir)
            
        if len(safety_cases) > 0:
            logger.info("å¼€å§‹è¯„æµ‹å®‰å…¨é—®é¢˜ç±»åˆ«")
            self._evaluate_new_class_cases("å®‰å…¨é—®é¢˜", safety_cases, model_interface, output_dir)
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        final_results = self._calculate_final_scores()
        
        # ç”ŸæˆæŠ¥å‘Š
        report_path = os.path.join(output_dir, "evaluation_report.html")
        self.report_generator.generate_report(
            final_results, 
            self.detailed_results, 
            report_path,
            general_assessment_task_results=self.general_assessment_task_results
        )
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_path = os.path.join(output_dir, "detailed_results.json")
        with open(results_path, 'w', encoding='utf-8') as f:
            # åœ¨å†™å…¥JSONå‰ç»Ÿä¸€å°†æ‰€æœ‰åˆ†æ•°å››èˆäº”å…¥è‡³å°æ•°ç‚¹å4ä½
            def r4(x: float) -> float:
                try:
                    return float(f"{float(x):.4f}")
                except Exception:
                    return x

            rounded_dimension_scores = {k: r4(v) for k, v in final_results.get("dimension_scores", {}).items()}
            rounded_general_tasks = {k: r4(v) for k, v in final_results.get("general_assessment_tasks", {}).items()}

            # é€æ¡ˆä¾‹ç»´åº¦åˆ†æ•°ä¿ç•™4ä½å°æ•°
            rounded_detailed_results = []
            for cr in self.detailed_results:
                cr_copy = cr.copy()
                ds = cr_copy.get("dimension_scores", {}) or {}
                cr_copy["dimension_scores"] = {k: r4(v) for k, v in ds.items()}
                rounded_detailed_results.append(cr_copy)

            # é€šç”¨è¯„æµ‹ä»»åŠ¡çš„è¯¦ç»†ç»“æœåŒæ ·å¤„ç†
            rounded_general_task_detailed_results = {}
            for class_name, items in self.general_assessment_task_results.items():
                new_list = []
                for cr in items:
                    cr_copy = cr.copy()
                    ds = cr_copy.get("dimension_scores", {}) or {}
                    cr_copy["dimension_scores"] = {k: r4(v) for k, v in ds.items()}
                    # è¾“å‡ºä¸­æ›¿æ¢æ˜¾ç¤ºçš„ç±»åˆ«å
                    if "class" in cr_copy:
                        cr_copy["class"] = DISPLAY_NAME_MAP.get(cr_copy["class"], cr_copy["class"])
                    new_list.append(cr_copy)
                if new_list:  # åªåŒ…å«æœ‰ç»“æœçš„ç±»åˆ«
                    out_key = DISPLAY_NAME_MAP.get(class_name, class_name)
                    rounded_general_task_detailed_results[out_key] = new_list

            # é‡æ„è¾“å‡ºç»“æ„ï¼Œé¿å…æ•°æ®å†—ä½™
            # åœ¨å†™å‡ºé…ç½®å‰é®è”½æ•æ„Ÿå­—æ®µä¸å†…éƒ¨å­—æ®µ
            sanitized_config = {
                "data_path": self.config.data_path,
                "local_model_gpu_id": self.config.local_model_gpu_id,
                "reward_api_host": self.config.reward_api_host,
                "reward_api_port": self.config.reward_api_port,
                "llm_judge_api_host": self.config.llm_judge_api_host,
                "llm_judge_api_port": self.config.llm_judge_api_port,
                "reward_model_name": self.config.reward_model_name,
                "llm_judge_model_name": self.config.llm_judge_model_name,
                "max_retries": self.config.max_retries,
                "checkpoint_interval": self.config.checkpoint_interval,
                # ä¸è¾“å‡ºï¼šAPI Keysã€stop_on_model_errorã€fatal_error_keywords ç­‰å†…éƒ¨æ§åˆ¶é¡¹
            }

            # è®¡ç®— SDT_task.scoreï¼ˆç­‰ä»·äºå¯¹ dimension_scores çš„å‡å€¼ï¼‰
            try:
                sdt_mean = float(f"{np.mean(list(rounded_dimension_scores.values())) if rounded_dimension_scores else 0.0:.4f}")
            except Exception:
                sdt_mean = 0.0

            output_data = {
                "final_scores": {
                    "total_score": r4(final_results.get("total_score", 0.0)),
                    # SDT_task åŒºå—ç´§éš total_scoreï¼ŒåŒ…å«ç»´åº¦å¾—åˆ†ä¸å‡å€¼
                    "SDT_task": {
                        "dimension_scores": rounded_dimension_scores,
                        "score": sdt_mean
                    },
                    # å®é™…å‚ä¸è®¡åˆ†çš„ç±»åˆ«
                    "participating_classes": [DISPLAY_NAME_MAP.get(x, x) for x in final_results.get("participating_classes", [])],
                    # å°† num_cases æ”¾åœ¨ participating_classes ä¸‹æ–¹
                    "num_cases": final_results.get("num_cases", 0),
                    # é€šç”¨è¯„æµ‹ä»»åŠ¡ï¼ˆåŸ new_class_scoresï¼‰
                    "general_assessment_tasks": {DISPLAY_NAME_MAP.get(k, k): v for k, v in rounded_general_tasks.items()}
                },
                "config": sanitized_config,
                "random_seed": self.config.random_seed,
                "detailed_results": rounded_detailed_results,
                # é€šç”¨è¯„æµ‹ä»»åŠ¡çš„è¯¦ç»†ç»“æœ
                "general_assessment_task_detailed_results": rounded_general_task_detailed_results
            }
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è¯„æµ‹å®Œæˆï¼æ€»åˆ†ï¼š{final_results['total_score']:.4f}")
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{report_path}")
        
        return final_results

    def _is_fatal_error(self, err: Any) -> bool:
        """æ£€æŸ¥å¼‚å¸¸æˆ–é”™è¯¯æ¶ˆæ¯æ˜¯å¦å±äºè‡´å‘½é”™è¯¯ï¼ˆéœ€è¦ç«‹å³åœæ­¢è¯„æµ‹ï¼‰ã€‚"""
        try:
            msg = str(err or "")
            msg_lower = msg.lower()
            for kw in self.config.fatal_error_keywords:
                if kw and kw.lower() in msg_lower:
                    return True
        except Exception:
            return False
        return False

    def _evaluate_tcm_cases(self, cases: List[Dict], model_interface: ModelInterface, 
                           output_dir: str, resume_from_checkpoint: bool):
        """
        è¯„æµ‹ä¸­åŒ»è¾¨è¯è®ºæ²»ç±»åˆ«ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        """
        checkpoint_path = os.path.join(output_dir, "checkpoint.json")
        
        # è·å–å·²è¯„æµ‹çš„æ¡ˆä¾‹æ•°
        completed_cases = len(self.detailed_results)
        
        # åˆ›å»ºè¿›åº¦æ¡ï¼Œåˆå§‹ä½ç½®è®¾ç½®ä¸ºå·²è¯„æµ‹çš„æ¡ˆä¾‹æ•°
        pbar = tqdm(
            total=len(cases), 
            desc="è¯„æµ‹è¿›åº¦ - ä¸­åŒ»è¾¨è¯è®ºæ²»",
            position=0,
            leave=True,
            initial=completed_cases  # è®¾ç½®åˆå§‹å·²å®Œæˆçš„æ•°é‡
        )
        
        try:
            # é€æ¡ˆä¾‹è¯„æµ‹
            for case_idx in range(completed_cases, len(cases)):
                case = cases[case_idx]
                
                pbar.set_description(f"è¯„æµ‹æ¡ˆä¾‹ {case_idx + 1}/{len(cases)} - ä¸­åŒ»è¾¨è¯è®ºæ²»")
                
                # è¯„æµ‹å•ä¸ªæ¡ˆä¾‹
                case_result = self._evaluate_single_case(case, model_interface, pbar)
                self.detailed_results.append(case_result)
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(1)
                
                # å®šæœŸä¿å­˜æ–­ç‚¹
                if (case_idx + 1) % self.config.checkpoint_interval == 0:
                    checkpoint_data = {
                        "completed_cases": case_idx + 1,
                        "detailed_results": self.detailed_results,
                        "general_assessment_task_results": self.general_assessment_task_results,
                        "random_seed": self.config.random_seed,
                    }
                    save_checkpoint(checkpoint_data, checkpoint_path)
                    pbar.write(f"å·²ä¿å­˜æ–­ç‚¹ï¼šå®Œæˆ {case_idx + 1} ä¸ªä¸­åŒ»è¾¨è¯è®ºæ²»æ¡ˆä¾‹")
        
        except KeyboardInterrupt:
            logger.info("è¯„æµ‹è¢«ç”¨æˆ·ä¸­æ–­")
            # ä¿å­˜å½“å‰è¿›åº¦
            checkpoint_data = {
                "completed_cases": len(self.detailed_results),
                "detailed_results": self.detailed_results,
                "general_assessment_task_results": self.general_assessment_task_results,
                "random_seed": self.config.random_seed,
            }
            save_checkpoint(checkpoint_data, checkpoint_path)
            logger.info(f"å·²ä¿å­˜æ–­ç‚¹ï¼šå®Œæˆ {len(self.detailed_results)} ä¸ªä¸­åŒ»è¾¨è¯è®ºæ²»æ¡ˆä¾‹")
            raise

        except Exception as e:
            # å‘ç”Ÿè‡´å‘½é”™è¯¯æˆ–å…¶ä»–æœªå¤„ç†å¼‚å¸¸æ—¶ï¼Œä¿å­˜æ–­ç‚¹å¹¶åœæ­¢
            logger.error(f"è¯„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œæ­£åœ¨ä¿å­˜æ–­ç‚¹å¹¶åœæ­¢ï¼š{e}")
            checkpoint_data = {
                "completed_cases": len(self.detailed_results),
                "detailed_results": self.detailed_results,
                "general_assessment_task_results": self.general_assessment_task_results,
                "random_seed": self.config.random_seed,
            }
            save_checkpoint(checkpoint_data, checkpoint_path)
            raise       
        finally:
            pbar.close()

    def _evaluate_new_class_cases(self, class_name: str, cases: List[Dict], 
                                 model_interface: ModelInterface, output_dir: str):
        """
        è¯„æµ‹æ–°å¢ç±»åˆ«ï¼ˆä¸­åŒ»åŸºç¡€çŸ¥è¯†ã€åŒ»å­¦ä¼¦ç†ã€å®‰å…¨é—®é¢˜ï¼‰
        """
        checkpoint_path = os.path.join(output_dir, "checkpoint.json")
        
        # è·å–è¯¥ç±»åˆ«å·²è¯„æµ‹çš„æ¡ˆä¾‹æ•°
        completed_cases = len(self.general_assessment_task_results[class_name])
        
        # åˆ›å»ºè¿›åº¦æ¡ï¼Œåˆå§‹ä½ç½®è®¾ç½®ä¸ºå·²è¯„æµ‹çš„æ¡ˆä¾‹æ•°
        pbar = tqdm(
            total=len(cases), 
            desc=f"è¯„æµ‹è¿›åº¦ - {class_name}",
            position=0,
            leave=True,
            initial=completed_cases
        )
        
        try:
            # é€æ¡ˆä¾‹è¯„æµ‹
            for case_idx in range(completed_cases, len(cases)):
                case = cases[case_idx]
                
                pbar.set_description(f"è¯„æµ‹æ¡ˆä¾‹ {case_idx + 1}/{len(cases)} - {class_name}")
                
                # è¯„æµ‹å•ä¸ªæ¡ˆä¾‹
                case_result = self._evaluate_new_class_single_case(case, model_interface, pbar)
                self.general_assessment_task_results[class_name].append(case_result)
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(1)
                
                # å®šæœŸä¿å­˜æ–­ç‚¹
                if (case_idx + 1) % self.config.checkpoint_interval == 0:
                    checkpoint_data = {
                        "completed_cases": len(self.detailed_results),  # ä¿æŒä¸åŸæœ‰é€»è¾‘ä¸€è‡´
                        "detailed_results": self.detailed_results,
                        "general_assessment_task_results": self.general_assessment_task_results,
                        "random_seed": self.config.random_seed,
                    }
                    save_checkpoint(checkpoint_data, checkpoint_path)
                    pbar.write(f"å·²ä¿å­˜æ–­ç‚¹ï¼šå®Œæˆ {case_idx + 1} ä¸ª{class_name}æ¡ˆä¾‹")
        
        except KeyboardInterrupt:
            logger.info(f"{class_name}è¯„æµ‹è¢«ç”¨æˆ·ä¸­æ–­")
            # ä¿å­˜å½“å‰è¿›åº¦
            checkpoint_data = {
                "completed_cases": len(self.detailed_results),
                "detailed_results": self.detailed_results,
                "general_assessment_task_results": self.general_assessment_task_results,
                "random_seed": self.config.random_seed,
            }
            save_checkpoint(checkpoint_data, checkpoint_path)
            logger.info(f"å·²ä¿å­˜æ–­ç‚¹ï¼šå®Œæˆ {len(self.general_assessment_task_results[class_name])} ä¸ª{class_name}æ¡ˆä¾‹")
            raise

        except Exception as e:
            logger.error(f"{class_name}è¯„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œæ­£åœ¨ä¿å­˜æ–­ç‚¹å¹¶åœæ­¢ï¼š{e}")
            checkpoint_data = {
                "completed_cases": len(self.detailed_results),
                "detailed_results": self.detailed_results,
                "general_assessment_task_results": self.general_assessment_task_results,
                "random_seed": self.config.random_seed,
            }
            save_checkpoint(checkpoint_data, checkpoint_path)
            raise      
        finally:
            pbar.close()

    def _evaluate_new_class_single_case(self, case: Dict[str, Any], 
                                       model_interface: ModelInterface,
                                       pbar: tqdm) -> Dict[str, Any]:
        """
        è¯„æµ‹æ–°å¢ç±»åˆ«å•ä¸ªæ¡ˆä¾‹ï¼ˆä¸­åŒ»åŸºç¡€çŸ¥è¯†ã€åŒ»å­¦ä¼¦ç†ã€å®‰å…¨é—®é¢˜ï¼‰
        """
        case_id = case.get("id") or "unknown_case"
        
        case_result = {
            "case_id": case_id,
            "question": case["question"],
            "ground_truth": case["answer"],
            "question_type": case["question_type"],
            "options": case["option"],
            "model_responses": {},
            "dimension_scores": {},
            "detailed_evaluation_results": {},
            "class": case.get("class", "æœªçŸ¥ç±»åˆ«")  # æ·»åŠ ç±»åˆ«ä¿¡æ¯
        }
        
        # ä½¿ç”¨é€‰æ‹©é¢˜è¯„æµ‹å™¨è¿›è¡Œä¸‰è½®è¯„æµ‹
        try:
            scores, first_answers, detailed_results = self.multiple_choice_evaluator.evaluate_new_class(case, model_interface, pbar)
            case_result["dimension_scores"]["accuracy"] = scores.get("accuracy", 0.0)
            case_result["model_responses"]["answer"] = first_answers
            case_result["detailed_evaluation_results"] = detailed_results
            
        except Exception as e:
            logger.error(f"æ–°å¢ç±»åˆ«é€‰æ‹©é¢˜è¯„æµ‹å¤±è´¥: {e}")
            if self.config.stop_on_model_error and self._is_fatal_error(e):
                raise
            case_result["dimension_scores"]["accuracy"] = 0.0
            case_result["model_responses"]["answer"] = f"è¯„æµ‹å¤±è´¥: {str(e)}"
            case_result["detailed_evaluation_results"] = {"error": str(e)}
        
        return case_result

    def _evaluate_single_case(self, case: Dict[str, Any], 
                             model_interface: ModelInterface,
                             pbar: tqdm) -> Dict[str, Any]:
        """
        è¯„æµ‹å•ä¸ªæ¡ˆä¾‹ï¼ˆå›ºå®šé¡ºåº + åˆå¹¶é€‰æ‹©é¢˜è¯„æµ‹ï¼‰
        """
        # ç»Ÿä¸€case_idè·å–é€»è¾‘ï¼Œç¡®ä¿ä¸å…¶ä»–è¯„æµ‹å™¨ä¸€è‡´
        case_id = case.get("case_id") or case.get("id") or "unknown_case"

        case_result = {
            "case_id": case_id,
            "instruction": case["instruction"],
            "ground_truth": case["output"],
            "diagnosis": case["ä¸­åŒ»ç–¾ç—…è¯Šæ–­"],
            "dimension_scores": {},
            "model_responses": {},
            "detailed_evaluation_results": {}  # æ–°å¢ï¼šè¯¦ç»†è¯„æµ‹ç»“æœ
        }
        
        # 1) é€‰æ‹©é¢˜åˆå¹¶è¯„æµ‹ï¼ˆè¯å‹-ç—…æ€§-ç—…ä½-æ²»åˆ™æ²»æ³•ï¼‰
        try:
            scores_mc, first_answers, detailed_mc_results = self.multiple_choice_evaluator.evaluate_combined(case, model_interface, pbar)
            for dim in ["è¯å‹", "ç—…æ€§", "ç—…ä½", "æ²»åˆ™æ²»æ³•"]:
                case_result["dimension_scores"][dim] = scores_mc.get(dim, 0.0)
                # ä¸ºåç»­ä¾èµ–ï¼ˆè¯å‹ -> å¥–åŠ±æ¨¡å‹ï¼‰ï¼Œä»…è®°å½•ç¬¬ä¸€æ¬¡ï¼ˆåŸåºï¼‰ç­”æ¡ˆå­—æ¯ä¸²
                case_result["model_responses"][dim] = f"{dim}ç­”æ¡ˆï¼š" + first_answers.get(dim, "")
            
            # ä¿å­˜é€‰æ‹©é¢˜çš„ä¸‰è½®è¯„æµ‹è¯¦ç»†ç»“æœ
            case_result["detailed_evaluation_results"]["multiple_choice"] = detailed_mc_results
            
        except Exception as e:
            logger.error(f"é€‰æ‹©é¢˜åˆå¹¶è¯„æµ‹å¤±è´¥: {e}")
            if self.config.stop_on_model_error and self._is_fatal_error(e):
                raise
            for dim in ["è¯å‹", "ç—…æ€§", "ç—…ä½", "æ²»åˆ™æ²»æ³•"]:
                case_result["dimension_scores"][dim] = 0.0
                case_result["model_responses"][dim] = f"è¯„æµ‹å¤±è´¥: {str(e)}"
            case_result["detailed_evaluation_results"]["multiple_choice"] = {"error": str(e)}
        
        # 2) ç—…å› /ç—…æœºæ”¹ä¸ºLLMåˆ¤åˆ†
        try:
            scores_cm, responses_cm = self.llm_judge_evaluator.evaluate_cause_mechanism(case, model_interface, pbar)
            for dim in ["ç—…å› ", "ç—…æœº"]:
                case_result["dimension_scores"][dim] = scores_cm.get(dim, 0.0)
                case_result["model_responses"][dim] = responses_cm.get(dim, "")
        except Exception as e:
            logger.error(f"ç—…å› /ç—…æœº LLMè¯„æµ‹å¤±è´¥: {e}")
            if self.config.stop_on_model_error and self._is_fatal_error(e):
                raise
            for dim in ["ç—…å› ", "ç—…æœº"]:
                case_result["dimension_scores"][dim] = 0.0
                case_result["model_responses"][dim] = f"è¯„æµ‹å¤±è´¥: {str(e)}"
        
        # 3) å¤„æ–¹ç›¸å…³ç»´åº¦ï¼ˆæ–¹è¯å¥‘åˆåº¦-æ–¹å‰‚é…ä¼è§„å¾‹-é…ä¼ç¦å¿Œ-è¯æå®‰å…¨æ€§åˆ†æ-å¦Šå¨ ç¦å¿Œï¼‰
        try:
            scores_sp, responses_sp = self.reward_model_evaluator.evaluate_all(
                case, model_interface, first_answers.get("è¯å‹", ""), pbar, llm_judge_evaluator=self.llm_judge_evaluator
            )
            for dim in ["æ–¹è¯å¥‘åˆåº¦", "æ–¹å‰‚é…ä¼è§„å¾‹", "é…ä¼ç¦å¿Œ", "è¯æå®‰å…¨æ€§åˆ†æ", "å¦Šå¨ ç¦å¿Œ"]:
                case_result["dimension_scores"][dim] = scores_sp.get(dim, 0.0)
                if dim == "æ–¹è¯å¥‘åˆåº¦":
                    # å°†æ¨¡å‹è¾“å‡ºæ”¹å†™ä¸ºâ€œè¯ç‰©ç»„æˆåŠç”¨é‡â€é”®
                    case_result["model_responses"]["è¯ç‰©ç»„æˆåŠç”¨é‡"] = responses_sp.get(dim, "")
                else:
                    case_result["model_responses"][dim] = responses_sp.get(dim, "")
        except Exception as e:
            logger.error(f"å¥–åŠ±æ¨¡å‹è¯„æµ‹å¤±è´¥: {e}")
            if self.config.stop_on_model_error and self._is_fatal_error(e):
                raise
            for dim in ["æ–¹è¯å¥‘åˆåº¦", "æ–¹å‰‚é…ä¼è§„å¾‹", "é…ä¼ç¦å¿Œ", "è¯æå®‰å…¨æ€§åˆ†æ", "å¦Šå¨ ç¦å¿Œ"]:
                case_result["dimension_scores"][dim] = 0.0
                if dim == "æ–¹è¯å¥‘åˆåº¦":
                    case_result["model_responses"]["è¯ç‰©ç»„æˆåŠç”¨é‡"] = f"è¯„æµ‹å¤±è´¥: {str(e)}"
                else:
                    case_result["model_responses"][dim] = f"è¯„æµ‹å¤±è´¥: {str(e)}"
 
        # 4) LLMåˆ¤åˆ†ï¼ˆCoTå†…å®¹å®Œå¤‡æ€§-ç…æœæ–¹æ³•-æ³¨æ„äº‹é¡¹-éšç—‡åŠ å‡ï¼‰
        try:
            # ä½¿ç”¨æ•°æ®é›†ä¸­çš„æ ‡å‡†ç­”æ¡ˆä½œä¸ºç”Ÿæˆæ—¶æä¾›çš„å…³é”®ä¿¡æ¯ï¼Œé¿å…è¢«æµ‹æ¨¡å‹ç»“æœæ±¡æŸ“
            gt_prescription_herbs = case["output"].get("è¯ç‰©ç»„æˆåŠç”¨é‡", "")
            gt_syndrome = case["output"].get("è¯å‹", "")
            gt_treatment_principles = case["output"].get("æ²»åˆ™æ²»æ³•", "")

            scores_llm, responses_llm = self.llm_judge_evaluator.evaluate_all(
                case, model_interface, pbar,
                syndrome_choice=gt_syndrome,                      # æ³¨æ„äº‹é¡¹/éšç—‡åŠ å‡ï¼šæä¾›æ ‡å‡†è¯å‹
                prescription_herbs=gt_prescription_herbs,         # ä¸‰é¡¹ç”Ÿæˆä½¿ç”¨æ ‡å‡†å¤„æ–¹ç»„æˆ
                treatment_principles=gt_treatment_principles,     # ç…æœæ–¹æ³•ï¼šæä¾›æ ‡å‡†æ²»åˆ™æ²»æ³•
                skip_think=self.skip_think                        # ä¼ é€’skip_thinkå‚æ•°
            )
            
            # æ ¹æ®skip_thinkå‚æ•°å†³å®šè¦å¤„ç†çš„ç»´åº¦
            llm_dimensions = ["ç…æœæ–¹æ³•", "æ³¨æ„äº‹é¡¹", "éšç—‡åŠ å‡"]
            if not self.skip_think:
                llm_dimensions = ["CoTå†…å®¹å®Œå¤‡æ€§"] + llm_dimensions
            
            for dim in llm_dimensions:
                case_result["dimension_scores"][dim] = scores_llm.get(dim, 0.0)
                case_result["model_responses"][dim] = responses_llm.get(dim, "")
            
            # ğŸ†• æ–°å¢ï¼šCoTå‡†ç¡®æ€§ï¼ˆå¹»è§‰ï¼‰è¯„æµ‹
            if not self.skip_think and "CoTå†…å®¹å®Œå¤‡æ€§" in responses_llm:
                think_content = responses_llm["CoTå†…å®¹å®Œå¤‡æ€§"]
                
                # è°ƒç”¨å¹»è§‰è¯„æµ‹
                cot_accuracy_score, hallucination_details = \
                    self.llm_judge_evaluator.evaluate_hallucination(case, think_content, pbar)
                
                # ä¿å­˜åˆ†æ•°åˆ° dimension_scores
                case_result["dimension_scores"]["CoTå‡†ç¡®æ€§"] = cot_accuracy_score
                
                # ä¿å­˜å®Œæ•´ä¿¡æ¯åˆ° hallucination_details
                case_result["hallucination_details"] = hallucination_details
                
        except Exception as e:
            logger.error(f"LLMåˆå¹¶è¯„æµ‹å¤±è´¥: {e}")
            if self.config.stop_on_model_error and self._is_fatal_error(e):
                raise
            # æ ¹æ®skip_thinkå‚æ•°å†³å®šè¦å¤„ç†çš„ç»´åº¦
            llm_dimensions = ["ç…æœæ–¹æ³•", "æ³¨æ„äº‹é¡¹", "éšç—‡åŠ å‡"]
            if not self.skip_think:
                llm_dimensions = ["CoTå†…å®¹å®Œå¤‡æ€§", "CoTå‡†ç¡®æ€§"] + llm_dimensions
            
            for dim in llm_dimensions:
                case_result["dimension_scores"][dim] = 0.0
                # CoTå‡†ç¡®æ€§ä¸ä¿å­˜åœ¨model_responsesä¸­
                if dim != "CoTå‡†ç¡®æ€§":
                    case_result["model_responses"][dim] = f"è¯„æµ‹å¤±è´¥: {str(e)}"
        
        return case_result

    def _get_prescription_herbs_from_cache(self, case: Dict[str, Any]) -> str:
        """
        ä»å¥–åŠ±æ¨¡å‹è¯„æµ‹å™¨çš„å¤„æ–¹ç¼“å­˜ä¸­è·å–å¤„æ–¹ç»„æˆä¿¡æ¯

        Args:
            case: æ¡ˆä¾‹æ•°æ®

        Returns:
            å¤„æ–¹ç»„æˆå­—ç¬¦ä¸²
        """
        try:
            case_id = case.get("case_id") or case.get("id") or "unknown_case"
            if case_id in self.reward_model_evaluator.prescription_cache:
                prescription_response = self.reward_model_evaluator.prescription_cache[case_id].get("raw_response", "")
                # è§£æå¤„æ–¹ä¿¡æ¯
                prescription_info = self.reward_model_evaluator._parse_prescription_response(prescription_response)
                herbs_list = [f"{h[0]} {h[1]} {h[2]}".strip() for h in prescription_info.get("herbs", [])]
                return "\n".join(['- '+x for x in herbs_list]) if herbs_list else ""
            else:
                logger.warning(f"æœªæ‰¾åˆ°æ¡ˆä¾‹ {case_id} çš„å¤„æ–¹ç¼“å­˜")
                return ""
        except Exception as e:
            logger.error(f"è·å–å¤„æ–¹ç»„æˆå¤±è´¥: {e}")
            return ""

    def _get_syndrome_contents_from_detailed_results(self, detailed_mc_results: Dict[str, Any]) -> str:
        """
        ä»é€‰æ‹©é¢˜è¯¦ç»†ç»“æœä¸­è·å–è¯å‹çš„å…·ä½“å†…å®¹

        Args:
            detailed_mc_results: é€‰æ‹©é¢˜è¯¦ç»†è¯„æµ‹ç»“æœ

        Returns:
            è¯å‹å†…å®¹å­—ç¬¦ä¸²ï¼Œå¤šä¸ªè¯å‹ç”¨åˆ†å·åˆ†éš”
        """
        try:
            syndrome_mapping = detailed_mc_results.get("syndrome_mapping", {})
            letter_to_content = syndrome_mapping.get("letter_to_content", {})
            first_run_letters = syndrome_mapping.get("first_run_letters", [])

            # å°†å­—æ¯è½¬æ¢ä¸ºå…·ä½“å†…å®¹
            syndrome_contents = []
            for letter in first_run_letters:
                content = letter_to_content.get(letter, "")
                if content:
                    syndrome_contents.append(content)

            result = ";".join(syndrome_contents)
            logger.info(f"è¯å‹å†…å®¹: {result}")
            return result

        except Exception as e:
            logger.error(f"è·å–è¯å‹å†…å®¹å¤±è´¥: {e}")
            return ""

    def _get_treatment_principles_from_detailed_results(self, detailed_mc_results: Dict[str, Any]) -> str:
        """
        ä»é€‰æ‹©é¢˜è¯¦ç»†ç»“æœä¸­è·å–æ²»åˆ™æ²»æ³•çš„å…·ä½“å†…å®¹ï¼ˆç¬¬ä¸€æ¬¡åŸåºé€‰æ‹©ï¼‰

        Args:
            detailed_mc_results: é€‰æ‹©é¢˜è¯¦ç»†è¯„æµ‹ç»“æœ

        Returns:
            æ²»åˆ™æ²»æ³•å†…å®¹å­—ç¬¦ä¸²ï¼Œå¤šä¸ªç”¨åˆ†å·åˆ†éš”
        """
        try:
            tp_mapping = detailed_mc_results.get("treatment_principles_mapping", {})
            letter_to_content = tp_mapping.get("letter_to_content", {})
            first_run_letters = tp_mapping.get("first_run_letters", [])

            contents = []
            for letter in first_run_letters:
                content = letter_to_content.get(letter, "")
                if content:
                    contents.append(content)

            result = ";".join(contents)
            logger.info(f"æ²»åˆ™æ²»æ³•å†…å®¹: {result}")
            return result
        except Exception as e:
            logger.error(f"è·å–æ²»åˆ™æ²»æ³•å†…å®¹å¤±è´¥: {e}")
            return ""

    def _calculate_final_scores(self) -> Dict[str, Any]:
        """
        è®¡ç®—æœ€ç»ˆå¾—åˆ†
        
        Returns:
            æœ€ç»ˆå¾—åˆ†å­—å…¸
        """
        if not self.detailed_results and not any(self.general_assessment_task_results.values()):
            return {"total_score": 0.0, "dimension_scores": {}}
        
        result = {
            "total_score": 0.0,
            "dimension_scores": {},
            "general_assessment_tasks": {},
            "num_cases": len(self.detailed_results)  # åŸæœ‰ç±»åˆ«æ¡ˆä¾‹æ•°
        }
        
        # è®¡ç®—å„ç±»åˆ«çš„æ¡ˆä¾‹æ•°
        total_cases = len(self.detailed_results)
        for class_results in self.general_assessment_task_results.values():
            total_cases += len(class_results)
        
        result["num_cases"] = total_cases
        
        # è®¡ç®—ä¸­åŒ»è¾¨è¯è®ºæ²»ç±»åˆ«å¾—åˆ†ï¼ˆä¿æŒç°æœ‰é€»è¾‘ä½†æƒé‡ç›¸åŒï¼‰
        if self.detailed_results:
            # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
            dimension_avg_scores = {}
            for dimension in self.evaluation_dimensions:
                scores = [
                    result["dimension_scores"].get(dimension, 0.0) 
                    for result in self.detailed_results
                ]
                dimension_avg_scores[dimension] = np.mean(scores) if scores else 0.0
            
            # è®¡ç®—ä¸­åŒ»è¾¨è¯è®ºæ²»ç±»åˆ«æ€»åˆ†ï¼ˆå®é™…å‚ä¸ç»´åº¦å¹³å‡åˆ†çš„å¹³å‡å€¼ï¼Œæ ¹æ®skip_thinkå‚æ•°å¯èƒ½æ˜¯14æˆ–15ä¸ªç»´åº¦ï¼‰
            actual_dimension_count = len(dimension_avg_scores)
            tcm_score = np.mean(list(dimension_avg_scores.values())) if dimension_avg_scores else 0.0
            result["dimension_scores"] = dimension_avg_scores
            result["tcm_score"] = tcm_score
            
            # è®°å½•å®é™…å‚ä¸è®¡ç®—çš„ç»´åº¦æ•°é‡
            logger.info(f"ä¸­åŒ»è¾¨è¯è®ºæ²»ç±»åˆ«æ€»åˆ†è®¡ç®—: ä½¿ç”¨ {actual_dimension_count} ä¸ªç»´åº¦, å¹³å‡åˆ†: {tcm_score:.4f}")
            if self.skip_think:
                logger.info("å·²è·³è¿‡Thinkå†…å®¹å®Œå¤‡æ€§ç»´åº¦ï¼Œä½¿ç”¨14ç»´åº¦è®¡ç®—")
        else:
            tcm_score = 0.0
            result["tcm_score"] = tcm_score
        
        # è®¡ç®—æ–°å¢ç±»åˆ«çš„å¾—åˆ†ï¼ˆåªå¯¹æœ‰æ ·æœ¬çš„ç±»åˆ«è®¡å…¥æœ€ç»ˆå¹³å‡ï¼‰
        general_task_scores = {}
        participating_classes = []
        participating_scores = []

        # ä¸»ç±»å‚ä¸è®¡åˆ†
        if self.detailed_results:
            participating_classes.append("ä¸­åŒ»è¾¨è¯è®ºæ²»")
            participating_scores.append(tcm_score)

        # æ–°å¢ç±»åˆ«ï¼šè®°å½•æ¯ç±»å¹³å‡åˆ†ï¼Œä½†ä»…å°†â€œæœ‰æ ·æœ¬â€çš„ç±»åˆ«è®¡å…¥å‚ä¸å¹³å‡
        for class_name, results in self.general_assessment_task_results.items():
            if results:
                scores = [result["dimension_scores"].get("accuracy", 0.0) for result in results]
                # å‡†ç¡®ç‡å·²åœ¨[0,1]èŒƒå›´
                avg_score = np.mean(scores) if scores else 0.0
                out_name = DISPLAY_NAME_MAP.get(class_name, class_name)
                general_task_scores[out_name] = avg_score
                participating_classes.append(out_name)
                participating_scores.append(avg_score)

        result["general_assessment_tasks"] = general_task_scores
        result["participating_classes"] = participating_classes

        # ä»…å¯¹â€œå®é™…å‚ä¸è¯„æµ‹çš„ç±»åˆ«â€å–å¹³å‡
        total_score = np.mean(participating_scores) if participating_scores else 0.0
        result["total_score"] = total_score
        
        return result

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="BEST4SDTè¯„æµ‹")
    parser.add_argument("--model_type", choices=["api", "local"], required=True, help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--api_url", help="APIåœ°å€ï¼ˆAPIæ¨¡å¼ï¼‰")
    parser.add_argument("--model_name", help="æ¨¡å‹åç§°ï¼ˆAPIæ¨¡å¼ï¼‰")
    parser.add_argument("--api_key", help="API Keyï¼ˆAPIæ¨¡å¼ï¼‰")
    parser.add_argument("--model_path", help="æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰")
    parser.add_argument("--config_file", required=True, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--resume", action="store_true", help="ä»æ–­ç‚¹æ¢å¤")
    parser.add_argument("--skip_think", action="store_true", help="è·³è¿‡CoTå†…å®¹å®Œå¤‡æ€§è¯„æµ‹ï¼ˆé€‚ç”¨äºä¸æ”¯æŒCoTçš„æ¨¡å‹ï¼‰")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = EvaluationConfig()
    if os.path.exists(args.config_file):
        with open(args.config_file, 'r', encoding='utf-8') as f:
            config_dict = json.load(f)
            for key, value in config_dict.items():
                if hasattr(config, key):
                    setattr(config, key, value)
    
    # åˆ›å»ºæ¨¡å‹æ¥å£
    if args.model_type == "api":
        if not args.api_url or not args.model_name or not args.api_key:
            raise ValueError("APIæ¨¡å¼éœ€è¦æä¾›api_urlã€model_nameå’Œapi_key")
        from model_interface import APIModelInterface
        model_interface = APIModelInterface(args.api_url, args.model_name, args.api_key)
    else:  # local
        if not args.model_path:
            raise ValueError("æœ¬åœ°æ¨¡å¼éœ€è¦æä¾›model_path")
        from model_interface import LocalModelInterface
        model_interface = LocalModelInterface(args.model_path, config.local_model_gpu_id)
    
    # è¿è¡Œè¯„æµ‹
    benchmark = TCMBenchmark(config, skip_think=args.skip_think)
    results = benchmark.run_evaluation(model_interface, args.output_dir, args.resume)
    
    print(f"è¯„æµ‹å®Œæˆï¼æ€»åˆ†ï¼š{results['total_score']:.4f}")

if __name__ == "__main__":
    main()